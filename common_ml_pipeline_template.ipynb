{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVU0zLktYm0aayALb7oO3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pp-cat/ml_lab/blob/main/common_ml_pipeline_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B_HNABoeS9H"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\n",
        "from sklearn.preprocessing import KBinsDiscretizer, MultiLabelBinarizer, FunctionTransformer\n",
        "\n",
        "\n",
        "'''\n",
        " base grid search code\n",
        "'''\n",
        "def ml_grid_search(x_train, x_test, y_train, y_test, feature_engineering_pipeline):\n",
        "    '''\n",
        "    simple helper function to grid search an ExtraTreesClassifier model and\n",
        "    print out a classification report for the best param set.\n",
        "    Best here is defined as having the best cross-validated accuracy on the training set\n",
        "    '''\n",
        "\n",
        "    params = {  # some simple parameters to grid search\n",
        "        'max_depth': [10, None],\n",
        "        'n_estimators': [10, 50, 100, 500],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    }\n",
        "\n",
        "    base_model = ExtraTreesClassifier()\n",
        "    model_grid_search = GridSearchCV(base_model, param_grid=params, cv=3)\n",
        "\n",
        "    start_time = time.time()  # capture the start time\n",
        "\n",
        "    if feature_engineering_pipeline:  # fit FE pipeline to training data and use it to transform test data\n",
        "        parsed_x_train = feature_engineering_pipeline.fit_transform(x_train, y_train)\n",
        "        parsed_x_test = feature_engineering_pipeline.transform(x_test)\n",
        "    else:\n",
        "        parsed_x_train = x_train\n",
        "        parsed_x_test = x_test\n",
        "\n",
        "    parse_time = time.time()\n",
        "    print(f\"1) Parsing took {(parse_time - start_time):.2f} seconds\")\n",
        "\n",
        "    model_grid_search.fit(parsed_x_train, y_train)\n",
        "\n",
        "    fit_time = time.time()\n",
        "    print(f\"2) Training took {(fit_time - start_time):.2f} seconds\")\n",
        "\n",
        "    best_model = model_grid_search.best_estimator_\n",
        "\n",
        "    print(classification_report(y_true=y_test, y_pred=best_model.predict(parsed_x_test)))\n",
        "    end_time = time.time()\n",
        "    print(f\"3) Overall took {(end_time - start_time):.2f} seconds\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def make_train_test_dataset(df, response):\n",
        "    X, y = df.drop([response], axis=1), df[response]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        X, y, stratify=y, random_state=0, test_size=.2)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def value_counts_of_column_list(df, column_list):\n",
        "    '''\n",
        "    df: dataframe of pandas\n",
        "    column_list: list of columns of the df\n",
        "\n",
        "    It show value count of the selected columns(column_list) from df(pandas dataframe)\n",
        "    May work with function \"separate_column_by_type(df)\"\n",
        "    '''\n",
        "    for column in column_list:\n",
        "        print('==============')\n",
        "        print(column)\n",
        "        print('==============')\n",
        "        print(df[column].value_counts(dropna=False))\n",
        "\n",
        "def separate_column_by_type(df):\n",
        "    '''\n",
        "    return numerical type columns and categorical type columns\n",
        "    respectively of the dataframe\n",
        "    '''\n",
        "    numerical_types = ['float16', 'float32', 'float64', 'int16', 'int32', 'int64'] # the numeric types in Pandas\n",
        "    categorical_type = ['O'] # the object type in Pandas\n",
        "\n",
        "    numerical_columns = df.select_dtypes(include=numerical_types).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(include=categorical_type).columns.tolist()\n",
        "\n",
        "    return numerical_columns ,categorical_columns\n",
        "\n",
        "\n",
        "'''\n",
        " pipline construction\n",
        " change the pipepline to test the performance and evaluate the model !!\n",
        "'''\n",
        "categorical_00_pipeline = Pipeline(\n",
        "    [\n",
        "        ('select_categorical_00_features', FunctionTransformer(lambda df: df[categorical_columns])),\n",
        "        ('fill_na', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
        "        ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "numerical_00_pipeline = Pipeline(\n",
        "    [\n",
        "        ('select_numerical_00_features', FunctionTransformer(lambda df: df[numerical_columns])),\n",
        "        ('impute', SimpleImputer(strategy='median')),\n",
        "        ('scaler', RobustScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Union binary and numerical data set\n",
        "# please add additional pipeline if necessary !\n",
        "\n",
        "simple_fe = FeatureUnion(\n",
        "    [\n",
        "        ('categorical_00', categorical_00_pipeline),\n",
        "        ('numerical_00', numerical_00_pipeline)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# initial two required variables\n",
        "numerical_columns = []\n",
        "categorical_columns = []\n",
        "\n",
        "\n",
        "'''\n",
        " Simple exploratory data analysis rountine\n",
        "'''\n",
        "def simple_eda(df):\n",
        "    print(f\"Shape-> f{df.shape}\")\n",
        "    print(f\"\\n\\n--- Have a look about head Record ---\")\n",
        "    print(df.head())\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\n\\n--- info of dataframe ---\")\n",
        "    print(df.info())\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\n\\n--- null values mean distribution ---\")\n",
        "    print(df.isnull().mean())\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\n\\n--- overview statistic of dataframe ---\")\n",
        "    print(df.describe())\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    numerical_columns, categorical_columns = separate_column_by_type(df)\n",
        "    print(f\"\\n\\n--- categorical columns ---\")\n",
        "    print(df[categorical_columns].columns)\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\n\\n--- value counts of categorical columns ---\")\n",
        "    print(value_counts_of_column_list(df ,categorical_columns))\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\n\\n--- numerical columns ---\")\n",
        "    print(df[numerical_columns].columns)\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\n\\n--- value counts of numerical columns ---\")\n",
        "    print(value_counts_of_column_list(df ,numerical_columns))\n",
        "\n",
        "#\n",
        "# need to load csv before run first simple_eda() !\n",
        "#\n",
        "# pipeline need categorical_columns and numerical_columns to function !!\n",
        "#\n",
        "# subroutine need those two variables to separate pipeline for input and\n",
        "# add additional as required !!\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygXlpRwffsHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}