{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3b18d9-a72f-48fb-b538-9ae5813c7770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (6497, 13)\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  type  \n",
      "0      9.4        5     1  \n",
      "1      9.8        5     1  \n",
      "2      9.8        5     1  \n",
      "3      9.8        6     1  \n",
      "4      9.4        5     1  \n",
      "Train / Val / Test: 3897 1300 1300\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[32m     72\u001b[39m model = XGBRegressor(\n\u001b[32m     73\u001b[39m     objective=\u001b[33m\"\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     74\u001b[39m     n_estimators=\u001b[32m1000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m     eval_metric=\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     89\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 6. Predictions and evaluation on test set\u001b[39;00m\n\u001b[32m     92\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\wb\\WPy64-312101\\notebooks\\sklearn-env\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBModel.fit() got an unexpected keyword argument 'callbacks'"
     ]
    }
   ],
   "source": [
    "# xgboost_wine_quality.py\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Download datasets (red and white)\n",
    "BASE_DIR = \"./data_wine\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "urls = {\n",
    "    \"winequality-red.csv\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "    \"winequality-white.csv\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "}\n",
    "for fname, url in urls.items():\n",
    "    outpath = os.path.join(BASE_DIR, fname)\n",
    "    if not os.path.exists(outpath):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        urllib.request.urlretrieve(url, outpath)\n",
    "\n",
    "# 2. Load and combine (semicolon-separated CSVs)\n",
    "df_red = pd.read_csv(os.path.join(BASE_DIR, \"winequality-red.csv\"), sep=\";\")\n",
    "df_white = pd.read_csv(os.path.join(BASE_DIR, \"winequality-white.csv\"), sep=\";\")\n",
    "\n",
    "df_red[\"type\"] = 1  # red = 1\n",
    "df_white[\"type\"] = 0  # white = 0\n",
    "\n",
    "df = pd.concat([df_red, df_white], ignore_index=True)\n",
    "print(\"Combined shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# 3. Features and target\n",
    "target = \"quality\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target].astype(float)\n",
    "\n",
    "# 4. Train / validation / test split (60/20/20)\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.25, random_state=42)\n",
    "print(\"Train / Val / Test:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
    "\n",
    "# 5. Configure and train XGBoost regressor with early stopping\n",
    "\"\"\"\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=30,\n",
    "    verbose=50\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[xgb.callback.EarlyStopping(rounds=30, save_best=True)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# 6. Predictions and evaluation on test set\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE : {mae:.4f}\")\n",
    "print(f\"Test R2  : {r2:.4f}\")\n",
    "\n",
    "# 7. Feature importance (gain)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plot_importance(model, ax=ax, importance_type=\"gain\", show_values=True)\n",
    "ax.set_title(\"XGBoost feature importance (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Optional: SHAP explanations (install shap to use)\n",
    "try:\n",
    "    import shap\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP not available or failed:\", e)\n",
    "\n",
    "# 9. Example: get raw margins (untransformed predictions)\n",
    "raw_margin = model.predict(X_test, output_margin=True)\n",
    "print(\"Raw margin shape:\", raw_margin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878891c-c72b-46f5-8812-56fad579bdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sklearn-Env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
